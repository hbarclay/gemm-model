A simple model that estimates the performance of warp-specialized persistent GEMMs on GPUs. The model includes some analytical components as well as some empirical factors that could be tuned to different hardware or implementations by measuring real performance on silicon.

Also included is a script to collect some sample data from Cutlass CuTeDSL kernels on B200.

## Setup / Quick Start

The project has only been tested on a B200 machine with CUDA TK 13.0 and CUDA driver major version 580

```
git clone <repo>
cd <repo-name>
uv sync
```


## Testlists

One testlist is included. It consists of DeepSeek-V3 GEMM sizes generated by Grok for both fwd and bwd, with the sequence dim set to 4096.

## Collect Benchmark Data 

`benchmark_gemm.py` takes a testlist as input and sweeps across a list of mma shapes and cluster sizes for each input problem. For each problem shape, cluster shape, and mma shape, a warp-specialized persistent GEMM kernel from the cutlass CuTeDSL examples is benchmarked.

Some simplifying assumptions were made in this script:
- Only 2CTA instructions are covered
- Only a subset of the valid tile sizes are tested
- e5m2 is not covered
- M,N, and K are all padded to 16-byte-aligned sizes
- All kernels are run with k-major input, n-major output layouts


Don't forget to lock clocks before benchmarking.
```
nvidia-smi -pm 1
nvidia-smi -lgc 1300,1300
uv run benchmark_gemm.py --input_csv=testlists/dsv3.csv --dtype=nvfp4 --output_csv=results/dsv3_nvfp4.csv
```

## Model Configuration

Model configuration defaults can be found in `config/`

## Get Model Predictions

```
uv run predict_gemm.py +input_csv=results/dsv3_nvfp4.csv +output_dir=results/nvfp4/
```

This will produce an output csv and a bar plot

## Implemented GEMM Models

### SOL Model

The SOL represents the "speed of light" performance based on the hardware peak math and dram throughputs. The runtime is computed as:

```math
runtime = \max(\mathrm{math}, \mathrm{DRAM})
```
where $\mathrm{math}$ is the SOL time to do $2 \cdot M \cdot N \cdot K$ ops, and $\mathrm{DRAM}$ is the SOL time to read A and B, and write C.


### WSPersistentGEMM Model

The WSPersistentGEMM model represents the kernel as a sequence of phases, where each phase consists of overlapping workloads corresponding to the specialized task of each warp. At steady-state, the "mainloop" consists of overlapping DMA (memory read), Math (MMA), and Epilogue (SIMT and memory write) workloads.

Because the workload for each output tile is the same, all mainloop iterations will have the same limiter. The prologue consists of any launch latency as well as the time to complete the load the first input tiles. Note that on the last wave, the epilogue will always be exposed.

![Blackwell Persistent Warp-Specialized GEMM Design: view from a single CTA for 4 waves
](img/blackwell_gemm.png)

Though not represented in the image, this model also considers the pipelining of the K-loop; that is, the pipelining of loads and MMAs along the inner (K-dimension), or the inner loop of the GEMM.

This model computes the runtime as:

```math
runtime = \mathrm{launch overhead} + \mathrm{first DMA} + \mathrm{mainloop} + \mathrm{last wave epilogue}
```
where $\mathrm{mainloop}$ is $\max(\mathrm{DMA}, \mathrm{Math}, \mathrm{epilogue})$ for each wave.

The epilogue is modeled as a constant overhead combined with the time to write the output.

The DMA workload for each SM is modeled as the total load bytes (including both the input tile and scale factors for block-scaled inputs) divided by the cluster size in the multicast dimension for each input tensor (n for A, and m for B). 
For simplicity, in/out layouts and related kernel functionality are not considered.

## Results



